<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150224311-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-150224311-1');
    </script>


    <title>MIC: Mining Interclass Characteristics for Improved Metric Learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta property="og:image" content="images/eccv_pipeline_diagram_new_symbols_v2_4.jpg">
    <meta property="og:title" content="A Style-Aware Content Loss for Real-time HD Style Transfer">


    <script type="text/javascript">
        // redefining default features
        var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
    </script>
    <link media="all" href="css/mainstyles.css" type="text/css" rel="StyleSheet">
    <style type="text/css" media="all">
        IMG {
            PADDING-RIGHT: 0px;
            PADDING-LEFT: 0px;
            FLOAT: right;
            PADDING-BOTTOM: 0px;
            PADDING-TOP: 0px
        }
        #primarycontent {
            MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
        1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
        1000px }
        BODY {
            TEXT-ALIGN: center
        }
        table.comparison td {
            PADDING-LEFT: 0px; PADDING-BOTTOM: 3px; BORDER-COLLAPSE: collapse; border-spacing: 0;
            width: 11.1111111%
        }
        td.ours {
          border-right: solid 0px #000; 
          border-left: solid 0px #000;
          padding-left: 0px;
          padding-right: 0px;
        }
    </style>
</head>

<body>

<div id="primarycontent">
<center><h1>MIC: Mining Interclass Characteristics for Improved Metric Learning</h1></center>
<center><h2><a href="https://karroth.com//">Karsten Roth</a>*&nbsp;&nbsp;&nbsp;
  <a href="http://bbrattoli.github.io">Biagio Brattoli</a>*&nbsp;&nbsp;&nbsp;
  <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Björn Ommer</a></h2></center>
<center><h2><a href="https://hci.iwr.uni-heidelberg.de/compvis">Heidelberg University</a></h2></center>
<center><h2>In ICCV 2019</h2></center>
<center><h2><strong><a href="https://arxiv.org/abs/1909.11574">Paper</a> |  <a href="https://github.com/Confusezius/metric-learning-mining-interclass-characteristics">Code</a></strong> </h2></center>
<center style="overflow:hidden;">
<!--    <a href="images/teaser_eccv18.jpg">-->

    <img src="images/model/page1.png" width="350" class="center" style="padding-top:10px">
    <img src="images/model/model_simple.png" width="600" class="center" style="padding-top:50px">

<!--    </a>-->
    </center>
<p></p>


 <p>
</p><h2>Abstract</h2>

<div style="font-size:14px"><p>Metric learning seeks to embed images of objects such
that class-defined relations are captured by the embedding
space. However, variability in images is not just due to different
depicted object classes, but also depends on other
latent characteristics such as viewpoint or illumination. In
addition to these structured properties, random noise further
obstructs the visual relations of interest. The common
approach to metric learning is to enforce a representation
that is invariant under all factors but the ones of interest. In
contrast, we propose to explicitly learn the latent characteristics
that are shared by and go across object classes. We
can then directly explain away structured visual variability,
rather than assuming it to be unknown random noise.
We propose a novel surrogate task to learn visual characteristics
shared across classes with a separate encoder.
This encoder is trained jointly with the encoder for class
information by reducing their mutual information. On five
standard image retrieval benchmarks the approach significantly
improves upon the state-of-the-art.</p></div>

<a href="https://arxiv.org/abs/1909.11574"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper_thumbnail.png" width="170"></a>
<br>



<h2>Paper</h2>
<p><a href="https://arxiv.org/abs/1909.11574">arxiv 1909.11574</a>,  2019.</p>


<!--
<h2>Citation</h2>
<p>Karsten Roth*, Biagio Brattoli*, Björn Ommer.
"MIC: Mining Interclass Characteristics for Improved Metric Learning", in International Conference on Computer Vision (ICCV),
2019.
<br>(* indicates equal contributions)
<a href="bibtex.txt">Bibtex</a>

</p>
-->
<a href="bibtex.txt"><h2>Citation</h2></a>
<pre>
@inproceedings{mic,
  title={MIC: Mining Interclass Characteristics for Improved Metric Learning},
  author={Roth, Karsten and Brattoli, Biagio and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={8000--8009},
  year={2019}
}
</pre>
    


<h2>Code and models: <a href="https://github.com/Confusezius/metric-learning-mining-interclass-characteristics">PyTorch</a>
    <br>
Poster: <a href="">poster.pdf</a></h2>
<br>

<h1 align="center">Experiments</h1>

<h2>Learned Embedding</h2>
<p>
    <img src="images/qualitative/class_umap_horizontal.png" width="350" style="padding: 25px 50px 75px 100px;">
    <img src="images/qualitative/intraclass_umap_horizontal.png" width="350" style="padding: 25px 50px 75px 100px;">
    UMAP projection for CARS196 of learned class specific embedding (left) and inter-class embedding (right).
    Seven clusters are selected, showing six images near the centroid and their ground-truth labels.
    We see that the encoding extracts class pecific information and ignores other (e.g. orientation) in the left embedding.
    On the other hand, the inter-class embedding (right) learns to ignore class specific features.
</p>

<h2>Comparison with SOTA</h2>
<p>
    <img src="images/tables/CARS196.png" width="350" style="padding: 25px 50px 75px 100px;">
    <img src="images/tables/CUB200-2011.png" width="350" style="padding: 25px 50px 75px 100px;">
</p>

<p>
    <img src="images/tables/SOP.png" width="350" style="padding: 25px 50px 75px 100px;">
    <img src="images/tables/In-Shop.png" width="350" style="padding: 25px 50px 75px 100px;">
    <img src="images/tables/VehicleID.png" width="350" style="padding: 25px 50px 75px 100px;">
</p>

<!--<div id="references">-->
<!--<h2>References</h2>-->
<!--[1] Leon Gatys, Alexander Ecker, Matthias Bethge "Image style transfer using convolutional neural networks", in CVPR 2016.<br>-->
<!--[2] Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros <a href="https://arxiv.org/abs/1703.10593">"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"</a>, in ICCV 2017.<br>-->
<!--[3] Tian Qi Chen, Mark Schmidt <a href="https://arxiv.org/abs/1612.04337">"Fast patch-based style transfer of arbitrary style"</a>, arXiv:1612.04337 <br>-->
<!--[4] Xun Huang, Serge Belongie "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization", in ICCV 2017.<br>-->
<!--[5] Li, Y., Fang, C., Yang, J., Wang, Z., Lu, X., Yang, M.H. "Universal style transfer via feature transforms", in NIPS 2017.<br>-->
<!--[6] Justin Johnson,  Alexandre Alahi, Li Fei-Fei "Perceptual losses for real-time style transfer and super-resolution", in ECCV 2016.<br>-->
<!--</div>-->
    
    
<p>&nbsp;</p>
</div></body></html>
